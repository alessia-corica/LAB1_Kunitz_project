{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# HMM Profile Construction for Kunitz Domain Proteins\n",
    "**AUTHOR:** Alessia Corica\\\n",
    "**PROJECT:** LAB1 Kunitz project\\\n",
    "**GOAL:** Extract, cluster, and model Kunitz domain sequences from UniProt and PDB to build a profile HMM."
   ],
   "id": "a7df3f63001bebfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Prepare UniProt and PDB data\n",
    "\n",
    "- Extract human and non-human Kunitz sequences from `all_kunitz.fasta`\n",
    "- Convert custom PDB CSV into FASTA format\n",
    "- Count entries and store for further processing\n"
   ],
   "id": "7903466237bf32e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract human sequences (Homo sapiens)\n",
    "awk '/^>/ {f=($0 ~ /OS=Homo sapiens/)} f' all_kunitz.fasta > human_kunitz_sequences.fasta\n",
    "\n",
    "# Extract non-human sequences\n",
    "grep -v \"Homo sapiens\" all_kunitz.fasta > non_human_kunitz_sequences.fasta\n",
    "\n",
    "# Convert PDB CSV to FASTA (PF00014 only)\n",
    "cat rcsb_pdb_custom_report_*.csv | tr -d '\"' | awk -F ',' '{if (length($2)>0) {name=$2}; print name ,$3,$4,$5}' | grep PF00014 | awk '{print \">\"$1\"_\"$3; print $2}' > pdb_kunitz_customreported.fasta\n"
   ],
   "id": "371297abd73950bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Cluster sequences and remove redundancy\n",
    "\n",
    "- Cluster sequences at 90% identity using `cd-hit`\n",
    "- Remove unwanted sequences\n",
    "- Extract representative sequences using `.clstr`\n"
   ],
   "id": "e17962391bad2f0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cluster sequences at 90% identity\n",
    "cd-hit -i pdb_kunitz_customreported.fasta -o pdb_kunitz_customreported_nr.fasta -c 0.9\n",
    "\n",
    "# Remove long sequence manually (2ODY_E in this case)\n",
    "awk '/^>2ODY_E/ {getline; next} {print}' pdb_kunitz_customreported_nr.fasta > pdb_kunitz_customreported_filtered.fasta\n",
    "\n",
    "# Extract representative sequences\n",
    "awk 'BEGIN{skip=0} /^>Cluster 0$/ {skip=1; next} /^>Cluster/ {skip=0} !skip' pdb_kunitz_customreported_nr.fasta.clstr > pdb_kunitz_customreported_filtered.clstr\n",
    "\n",
    "# Convert .clstr to readable txt\n",
    "clstr2txt.pl pdb_kunitz_customreported_filtered.clstr > pdb_kunitz.clusters.txt\n",
    "\n",
    "# Get representative IDs\n",
    "awk '$5 == 1 {print $1}' pdb_kunitz.clusters.txt > pdb_kunitz_rp.ids\n",
    "\n",
    "# Retrieve corresponding sequences\n",
    "for i in $(cat pdb_kunitz_rp.ids); do \\\n",
    "  grep -A 1 \"^>$i\" pdb_kunitz_customreported.fasta | head -n 2 >> pdb_kunitz_rp.fasta; \\\n",
    "done\n"
   ],
   "id": "2e8ff1d177e458b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d8ff19163be62908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Build the profile HMM\n",
    "\n",
    "- Format the FASTA file for HMMER\n",
    "- Build the HMM using `hmmbuild`\n"
   ],
   "id": "59749e26d3b585"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Format FASTA for HMMER (remove description, uppercase)\n",
    "awk '{ if (substr($1,1,1)==\">\") { print \"\\n\" toupper($1) } else { printf \"%s\", toupper($1) }}' pdb_kunitz_rp.fasta > pdb_kunitz_rp_formatted.ali\n",
    "\n",
    "# Build HMM profile\n",
    "hmmbuild structural_model.hmm pdb_kunitz_rp_formatted.ali"
   ],
   "id": "f65f8f1d2ab25cbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Create BLAST database and filter redundancy\n",
    "\n",
    "- Create a BLAST protein database from `kunitz_sequences.fasta`\n",
    "- Run BLAST (`blastp`) using the representative sequences as queries\n",
    "- Extract matching UniProt IDs with identity ≥ 95% and coverage ≥ 50%\n",
    "- Remove redundant entries and keep only non-redundant Kunitz sequences\n",
    "\n"
   ],
   "id": "a877b003670a7028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 4.1: Create BLAST database\n",
    "makeblastdb -in kunitz_sequences.fasta -dbtype prot -out kunitz_sequences.fasta\n",
    "\n",
    "# Step 4.2: Run BLAST alignment (23 clustered sequences against full set)\n",
    "blastp -query pdb_kunitz_rp.fasta -db kunitz_sequences.fasta -out pdb_kunitz_nr_23.blast -outfmt 7\n",
    "\n",
    "# Step 4.3: Extract matching UniProt IDs with identity ≥95% and coverage ≥50%\n",
    "grep -v \"^#\" pdb_kunitz_nr_23.blast | awk '{if ($3>=95 && $4>=50) print $2}' | sort -u | cut -d \"|\" -f 2 > to_remove.ids\n",
    "\n",
    "# Step 4.4: Get all original UniProt IDs from FASTA\n",
    "grep \">\" kunitz_sequences.fasta | cut -d \"|\" -f 2 > all_kunitz.id\n",
    "\n",
    "# Step 4.5: Remove redundant IDs (keep only those not matched)\n",
    "comm -23 <(sort all_kunitz.id) <(sort to_remove.ids) > to_keep.ids\n"
   ],
   "id": "ae6c4ebd3c19807f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Extract final non-redundant positive sequences\n",
    "\n",
    "- Use the `to_keep.ids` list (generated after BLAST filtering)\n",
    "- Extract corresponding sequences from `kunitz_sequences.fasta`\n",
    "- Use the custom script `get_seq.py`\n",
    "- Output: `ok_kunitz.fasta` → final set of non-redundant Kunitz domain proteins\n",
    "\n"
   ],
   "id": "b44ca52b866eb79e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract non-redundant Kunitz sequences\n",
    "python3 get_seq.py to_keep.ids kunitz_sequences.fasta ok_kunitz.fasta"
   ],
   "id": "c65be733adb8278b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Create the negative set\n",
    "\n",
    "- Extract all UniProt IDs from `uniprot_sprot.fasta`\n",
    "- Remove all sequences known to contain a Kunitz domain (stored in `all_kunitz.id`)\n",
    "- Save the remaining IDs to `sp_negs.ids`\n",
    "- Extract the sequences using `get_seq.py`\n"
   ],
   "id": "999b4143aa9b0e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract all UniProt IDs from SwissProt FASTA\n",
    "grep \">\" uniprot_sprot.fasta | cut -d\"|\" -f2 > sp.id\n",
    "\n",
    "# Remove IDs with Kunitz domain (PF00014) listed in all_kunitz.id\n",
    "comm -23 <(sort sp.id) <(sort all_kunitz.id) > sp_negs.ids\n",
    "\n",
    "# Extract final negative sequences\n",
    "python3 get_seq.py sp_negs.ids uniprot_sprot.fasta sp_negs.fasta\n"
   ],
   "id": "b69e5278461cb2c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7: Create train/test splits\n",
    "\n",
    "We now split the positive and negative datasets into two parts for evaluation.\n",
    "\n",
    "- The positive IDs are taken from the filtered non-redundant UniProt set (`random_ok_kunitz.ids`)\n",
    "- The negative IDs are selected from non-Kunitz SwissProt sequences (`random_sp_negs.ids`)\n",
    "- Each is split in half (using `head` and `tail`) to create balanced subsets:\n",
    "  - `pos_1.ids` / `pos_2.ids`\n",
    "  - `neg_1.ids` / `neg_2.ids`\n",
    "- Corresponding FASTA files are generated using `get_seq.py`\n"
   ],
   "id": "4f998fe32a92f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Shuffle IDs randomly\n",
    "sort -R to_keep.ids > random_ok_kunitz.ids\n",
    "sort -R sp_negs.ids > random_sp_negs.ids\n",
    "\n",
    "# Positive set (total: 366 IDs)\n",
    "head -n 183 random_ok_kunitz.ids > pos_1.ids\n",
    "tail -n 183 random_ok_kunitz.ids > pos_2.ids\n",
    "\n",
    "# Negative set (total: 572834 IDs)\n",
    "head -n 286417 random_sp_negs.ids > neg_1.ids\n",
    "tail -n 286417 random_sp_negs.ids > neg_2.ids\n",
    "\n",
    "# Extract sequences\n",
    "python3 get_seq.py pos_1.ids uniprot_sprot.fasta pos_1.fasta\n",
    "python3 get_seq.py pos_2.ids uniprot_sprot.fasta pos_2.fasta\n",
    "python3 get_seq.py neg_1.ids uniprot_sprot.fasta neg_1.fasta\n",
    "python3 get_seq.py neg_2.ids uniprot_sprot.fasta neg_2.fasta"
   ],
   "id": "574df4082d2d29b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8: Run `hmmsearch` and normalize\n",
    "\n",
    "\n",
    "- First, we perform a test with default settings (without normalization)\n",
    "- Then, we re-run `hmmsearch` with `-Z 1000` to normalize the E-values across datasets\n",
    "- Output: `.out` files for each test set (to be parsed in the next step)\n"
   ],
   "id": "30de8cc656d2b249"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run hmmsearch on positive and negative sets\n",
    "hmmsearch --max --tblout pos_1.out structural_model.hmm pos_1.fasta\n",
    "hmmsearch --max --tblout pos_2.out structural_model.hmm pos_2.fasta\n",
    "hmmsearch --max --tblout neg_1.out structural_model.hmm neg_1.fasta\n",
    "hmmsearch --max --tblout neg_2.out structural_model.hmm neg_2.fasta\n",
    "\n",
    "# Re-run hmmsearch with -Z 1000 to normalize E-values\n",
    "hmmsearch -Z 1000 --max --tblout pos_1.out structural_model.hmm pos_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout pos_2.out structural_model.hmm pos_2.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_1.out structural_model.hmm neg_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_2.out structural_model.hmm neg_2.fasta\n"
   ],
   "id": "811fc19d68e2ae18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 9: Parse `hmmsearch` outputs and build `.class` files\n",
    "\n",
    "- `.out` tables are converted into `.class` format for performance analysis\n",
    "- Each line in the `.class` file contains:\n",
    "  - UniProt ID\n",
    "  - Label (1 = positive, 0 = negative)\n",
    "  - Raw score\n",
    "  - E-value\n",
    "- Sequences not matched by `hmmsearch` are added with default score = 10.0 and E-value = 10.0 (fake hits)"
   ],
   "id": "a30e621b62ddcc90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parse matched hits\n",
    "grep -v \"^#\" pos_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_1.class\n",
    "grep -v \"^#\" pos_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_2.class\n",
    "\n",
    "grep -v \"^#\" neg_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_1.class\n",
    "grep -v \"^#\" neg_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_2.class\n",
    "\n",
    "# Add fake hits for missed negatives\n",
    "comm -23 <(sort neg_1.ids) <(cut -f1 neg_1.class | sort) | awk '{print $1\"\\t0\\t10.0\\t10.0\"}' >> neg_1.class\n",
    "comm -23 <(sort neg_2.ids) <(cut -f1 neg_2.class | sort) | awk '{print $1\"\\t0\\t10.0\\t10.0\"}' >> neg_2.class\n",
    "\n",
    "# Merge positive and negative class files\n",
    "cat pos_1.class neg_1.class > set_1.class\n",
    "cat pos_2.class neg_2.class > set_2.class"
   ],
   "id": "a5722ca6ba9edec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 10: Evaluate model performance\n",
    "\n",
    "- Evaluate the model using the `performance.py` script\n",
    "- Use a classification threshold of `1e-5`\n",
    "- Output metrics include:\n",
    "  - Accuracy\n",
    "  - Sensitivity\n",
    "  - Specificity\n"
   ],
   "id": "d297576132754448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run performance script on both sets\n",
    "python3 performance.py set_1.class 1e-5\n",
    "python3 performance.py set_2.class 1e-5"
   ],
   "id": "479f3164a068b8ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 12: Threshold exploration\n",
    "\n",
    "- Evaluate model performance across multiple thresholds\n",
    "- Save results ordered by MCC (Matthews Correlation Coefficient)\n",
    "- Extract worst false negatives for each set\n"
   ],
   "id": "ffa48c68d208fc70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate performance across thresholds for set_1\n",
    "for i in $(seq 1 10); do\n",
    "    python3 performance.py set_1.class 1e-$i\n",
    "done | sort -rk 6 > performance_set1_thresholds.txt\n",
    "\n",
    "# Evaluate performance across thresholds for set_2\n",
    "for i in $(seq 1 10); do\n",
    "    python3 performance.py set_2.class 1e-$i\n",
    "done | sort -rk 6 > performance_set2_thresholds.txt\n",
    "\n",
    "# Extract worst false negatives for comparison (e-value > 1e-5)\n",
    "awk '$2 == 1 && $3 > 1e-5' pos_1.class | sort -grk 3 > fn_pos1.txt\n",
    "awk '$2 == 1 && $3 > 1e-5' pos_2.class | sort -grk 3 > fn_pos2.txt"
   ],
   "id": "f9d3a4cfe08e9726"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
