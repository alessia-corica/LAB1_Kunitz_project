{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# HMM Profile Construction for Kunitz Domain Proteins\n",
    "**AUTHOR:** Alessia Corica\\\n",
    "**PROJECT:** LAB1 Kunitz project\\\n",
    "**GOAL:** Extract, cluster, and model Kunitz domain sequences from UniProt and PDB to build a profile HMM."
   ],
   "id": "a7df3f63001bebfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Prepare UniProt and PDB data\n",
    "\n",
    "- Extract human and non-human Kunitz sequences from `kunitz_sequences.fasta`\n",
    "- Convert custom PDB CSV into FASTA format to use `cd-hit` in the next step\n"
   ],
   "id": "7903466237bf32e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract human sequences (Homo sapiens)\n",
    "awk '/^>/ {f=($0 ~ /OS=Homo sapiens/)} f' kunitz_sequences.fasta > human_kunitz_sequences.fasta\n",
    "\n",
    "# Extract non-human sequences\n",
    "grep -v \"Homo sapiens\" kunitz_sequences.fasta > nothuman_kunitz_sequences.fasta\n",
    "\n",
    "# Convert PDB CSV to FASTA (PF00014 only)\n",
    "cat rcsb_pdb_custom_report_20250505025420.csv | tr -d '\"' | awk -F ',' '{if (length($2)>0) {name=$2}; print name ,$3,$4,$5}' | grep PF00014 | awk '{print \">\"$1\"_\"$3; print $2}' > pdb_kunitz_customreported.fasta\n"
   ],
   "id": "371297abd73950bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Cluster sequences and remove redundancy\n",
    "\n",
    "- Cluster sequences at 90% identity using `cd-hit`\n",
    "- Filter out sequences not suitable for modeling\n",
    "- Select one representative sequence per cluster to obtain a non-redundant dataset using `.clstr`\n"
   ],
   "id": "e17962391bad2f0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cluster sequences at 90% identity\n",
    "cd-hit -i pdb_kunitz_customreported.fasta -o pdb_kunitz_customreported_nr.fasta -c 0.9\n",
    "\n",
    "# Remove outliers (2ODY_E in this case)\n",
    "awk '/^>2ODY_E/ {getline; next} {print}' pdb_kunitz_customreported_nr.fasta > pdb_kunitz_customreported_filtered.fasta\n",
    "\n",
    "# Repeat the previous step also for the corresponding .clstr file\n",
    "awk 'BEGIN{skip=0} /^>Cluster 0$/ {skip=1; next} /^>Cluster/ {skip=0} !skip' pdb_kunitz_customreported_nr.fasta.clstr > pdb_kunitz_customreported_filtered.clstr\n",
    "\n",
    "# Convert .clstr to readable text file\n",
    "clstr2txt.pl pdb_kunitz_customreported_filtered.clstr > pdb_kunitz.clusters.txt\n",
    "\n",
    "# Get representative IDs\n",
    "awk '$5 == 1 {print $1}' pdb_kunitz.clusters.txt > pdb_kunitz_rp.ids\n",
    "\n",
    "# Retrieve corresponding sequences\n",
    "for i in $(cat pdb_kunitz_rp.ids); do \\\n",
    "  grep -A 1 \"^>$i\" pdb_kunitz_customreported.fasta | head -n 2 >> pdb_kunitz_rp.fasta; \\\n",
    "done\n"
   ],
   "id": "2e8ff1d177e458b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Build the profile HMM\n",
    "\n",
    "- Format FASTA for HMMER: remove header description and convert sequences to uppercase\n",
    "- Build the HMM profile using `hmmbuild`\n"
   ],
   "id": "59749e26d3b585"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Format FASTA for HMMER\n",
    "awk '{ if (substr($1,1,1)==\">\") { print \"\\n\" toupper($1) } else { printf \"%s\", toupper($1) }}' pdb_kunitz_rp.fasta > pdb_kunitz_rp_formatted.ali\n",
    "\n",
    "# Build HMM profile\n",
    "hmmbuild structural_model.hmm pdb_kunitz_rp_formatted.ali"
   ],
   "id": "f65f8f1d2ab25cbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Create BLAST database and filter redundancy\n",
    "\n",
    "- Create a BLAST protein database from `kunitz_sequences.fasta`\n",
    "- Run BLAST (`blastp`) using the representative sequences as queries\n",
    "- Detect and remove UniProt sequences highly similar to the training set (≥95% identity, ≥50% coverage)\n",
    "\n"
   ],
   "id": "a877b003670a7028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create BLAST database\n",
    "makeblastdb -in kunitz_sequences.fasta -dbtype prot -out kunitz_sequences.fasta\n",
    "\n",
    "# Run BLAST alignment\n",
    "blastp -query pdb_kunitz_rp.fasta -db kunitz_sequences.fasta -out pdb_kunitz_nr_23.blast -outfmt 7\n",
    "\n",
    "# Extract matching UniProt IDs with identity ≥95% and coverage ≥50%\n",
    "grep -v \"^#\" pdb_kunitz_nr_23.blast | awk '{if ($3>=95 && $4>=50) print $2}' | sort -u | cut -d \"|\" -f 2 > to_remove.ids\n",
    "\n",
    "# Get all original UniProt IDs from FASTA\n",
    "grep \">\" kunitz_sequences.fasta | cut -d \"|\" -f 2 > all_kunitz.id\n",
    "\n",
    "# Remove redundant IDs\n",
    "comm -23 <(sort all_kunitz.id) <(sort to_remove.ids) > to_keep.ids\n"
   ],
   "id": "ae6c4ebd3c19807f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Extract final non-redundant positive sequences\n",
    "\n",
    "- Use the list of non-redundant UniProt IDs (`to_keep.ids`) to extract the corresponding sequences from `kunitz_sequences.fasta`.\n",
    "- This produces `ok_kunitz.fasta`, the final FASTA file containing only the filtered Kunitz domain proteins.\n",
    "\n",
    "\n"
   ],
   "id": "b44ca52b866eb79e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract non-redundant Kunitz sequences\n",
    "python3 get_seq.py to_keep.ids kunitz_sequences.fasta ok_kunitz.fasta"
   ],
   "id": "c65be733adb8278b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Create the negative set\n",
    "\n",
    "- Extract all UniProt IDs from `uniprot_sprot.fasta`\n",
    "- Remove IDs known to contain a Kunitz domain (stored in `all_kunitz.id`)\n",
    "- Save the remaining IDs to `sp_negs.ids` and extract the sequences using `get_seq.py`"
   ],
   "id": "999b4143aa9b0e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract all UniProt IDs from SwissProt FASTA\n",
    "grep \">\" uniprot_sprot.fasta | cut -d\"|\" -f2 > sp.id\n",
    "\n",
    "# Remove IDs with Kunitz domain (PF00014)\n",
    "comm -23 <(sort sp.id) <(sort all_kunitz.id) > sp_negs.ids\n",
    "\n",
    "# Extract final negative sequences\n",
    "python3 get_seq.py sp_negs.ids uniprot_sprot.fasta sp_negs.fasta\n"
   ],
   "id": "b69e5278461cb2c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7: Create train/test splits\n",
    "\n",
    "- Split positive and negative IDs into train/test halves for evaluation using `head`/`tail`\n",
    "- Generate corresponding FASTA files using `get_seq.py`"
   ],
   "id": "4f998fe32a92f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Shuffle IDs randomly\n",
    "sort -R to_keep.ids > random_ok_kunitz.ids\n",
    "sort -R sp_negs.ids > random_sp_negs.ids\n",
    "\n",
    "# Positive set (total: 366 IDs)\n",
    "head -n 183 random_ok_kunitz.ids > pos_1.ids\n",
    "tail -n 183 random_ok_kunitz.ids > pos_2.ids\n",
    "\n",
    "# Negative set (total: 572834 IDs)\n",
    "head -n 286417 random_sp_negs.ids > neg_1.ids\n",
    "tail -n 286417 random_sp_negs.ids > neg_2.ids\n",
    "\n",
    "# Extract sequences\n",
    "python3 get_seq.py pos_1.ids uniprot_sprot.fasta pos_1.fasta\n",
    "python3 get_seq.py pos_2.ids uniprot_sprot.fasta pos_2.fasta\n",
    "python3 get_seq.py neg_1.ids uniprot_sprot.fasta neg_1.fasta\n",
    "python3 get_seq.py neg_2.ids uniprot_sprot.fasta neg_2.fasta"
   ],
   "id": "574df4082d2d29b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8: Run `hmmsearch` and normalize\n",
    "\n",
    "- Run `hmmsearch` on each test set\n",
    "- Repeat with `-Z 1000` to normalize E-values across datasets"
   ],
   "id": "30de8cc656d2b249"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run hmmsearch on positive and negative sets\n",
    "hmmsearch --max --tblout pos_1.out structural_model.hmm pos_1.fasta\n",
    "hmmsearch --max --tblout pos_2.out structural_model.hmm pos_2.fasta\n",
    "hmmsearch --max --tblout neg_1.out structural_model.hmm neg_1.fasta\n",
    "hmmsearch --max --tblout neg_2.out structural_model.hmm neg_2.fasta\n",
    "\n",
    "# Repeat hmmsearch with -Z 1000 to normalize E-values across datasets\n",
    "hmmsearch -Z 1000 --max --tblout pos_1.out structural_model.hmm pos_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout pos_2.out structural_model.hmm pos_2.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_1.out structural_model.hmm neg_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_2.out structural_model.hmm neg_2.fasta\n"
   ],
   "id": "811fc19d68e2ae18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 9: Parse `hmmsearch` outputs and build `.class` files\n",
    "\n",
    "- Build `.class` files with ID, label, score, and E-value\n",
    "- Add sequences not matched by `hmmsearch`"
   ],
   "id": "a30e621b62ddcc90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract matched hits from hmmsearch outputs and format as .class\n",
    "grep -v \"^#\" pos_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_1.class\n",
    "grep -v \"^#\" pos_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_2.class\n",
    "\n",
    "grep -v \"^#\" neg_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_1.class\n",
    "grep -v \"^#\" neg_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_2.class\n",
    "\n",
    "# Add missing negatives\n",
    "comm -23 <(sort neg_1.ids) <(cut -f1 neg_1.class | sort) | awk '{print $1\"\\t0\\t10.0\\t10.0\"}' >> neg_1.class\n",
    "comm -23 <(sort neg_2.ids) <(cut -f1 neg_2.class | sort) | awk '{print $1\"\\t0\\t10.0\\t10.0\"}' >> neg_2.class\n",
    "\n",
    "# Combine positive and negative entries into final test sets\n",
    "cat pos_1.class neg_1.class > set_1.class\n",
    "cat pos_2.class neg_2.class > set_2.class"
   ],
   "id": "a5722ca6ba9edec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 10: Evaluate model performance\n",
    "\n",
    "- Evaluate the model using the `performance.py` script using a classification threshold of `1e-5`\n",
    "- Output metrics include:\n",
    "  - Accuracy\n",
    "  - Sensitivity\n",
    "  - Specificity\n"
   ],
   "id": "d297576132754448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run performance script on both sets\n",
    "python3 performance.py set_1.class 1e-5\n",
    "python3 performance.py set_2.class 1e-5"
   ],
   "id": "479f3164a068b8ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 12: Threshold exploration\n",
    "\n",
    "- Evaluate model performance on thresholds 1e-1 to 1e-10\n",
    "- Extract false negatives and false positives for each set"
   ],
   "id": "ffa48c68d208fc70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate performance across thresholds for each set\n",
    "for i in $(seq 1 10); do\n",
    "    python3 performance.py set_1.class 1e-$i\n",
    "done > performance_set1.txt\n",
    "\n",
    "for i in $(seq 10 -1 1); do\n",
    "    python3 performance.py set_2.class 1e-$i\n",
    "done > performance_set2.txt\n",
    "\n",
    "# Extract false negatives (e-value > 1e-5)\n",
    "awk '$2 == 1 && $3 > 1e-5' pos_1.class | sort -grk 3 > fn_pos1.txt\n",
    "awk '$2 == 1 && $3 > 1e-5' pos_2.class | sort -grk 3 > fn_pos2.txt\n",
    "\n",
    "# Extract false positives (e-value < 1e-5)\n",
    "awk '$2 == 0 && $3 < 1e-5' neg_1.class | sort -grk 3 > fp_neg1.txt\n",
    "awk '$2 == 0 && $3 < 1e-5' neg_2.class | sort -grk 3 > fp_neg2.txt"
   ],
   "id": "f9d3a4cfe08e9726"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
